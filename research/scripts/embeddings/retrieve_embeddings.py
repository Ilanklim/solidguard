"""
retrieve_embeddings.py

Lightweight semantic search engine for the RAG knowledge store.

This script loads the JSONL embedding database generated by
build_knowledge_store.py and provides cosine-similarity retrieval over
all stored chunks.

Intended Use:
    • Query Solidity/security documentation (RAG_docs)
    • Retrieve the top-k most relevant knowledge chunks
    • Pass results into LLM prompts during contract analysis

Input:
    knowledge_store.jsonl
        One JSON per line containing:
            {
                "id": "...",
                "category": "...",
                "source_path": "...",
                "chunk_index": int,
                "text": "<chunk text>",
                "embedding": [float, float, ...]
            }

Responsibilities:
    • Load embeddings + metadata from the knowledge store
    • Normalize vectors (L2) for cosine similarity
    • Embed user queries using OpenAI embeddings
    • Return top-k highest-similarity chunks

Does NOT:
    • Modify or rebuild the knowledge store
    • Perform external RAG (just local vector search)
    • Apply reranking or hybrid search

Usage Example:
    store = KnowledgeStore()
    store.load()

    results = store.retrieve("What is a reentrancy attack?", k=5)
    for r in results:
        print(r["score"], r["category"], r["source"])
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Optional

import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

# CONFIG
ROOT = Path(__file__).resolve().parents[2]
STORE_PATH = ROOT / "knowledge_store.jsonl"
EMBED_MODEL = "text-embedding-3-large"

load_dotenv(ROOT / ".env")
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("Missing OPENAI_API_KEY in .env")

client = OpenAI(api_key=api_key)


# KNOWLEDGE STORE CLASS
class KnowledgeStore:
    """
    Loads all embeddings from knowledge_store.jsonl and enables
    cosine-similarity search across the entire vector space.
    """

    def __init__(self, path: Path = STORE_PATH):
        self.path = Path(path)
        self.records: List[Dict] = []
        self.embeddings: Optional[np.ndarray] = None

    # ------------------------ LOAD STORE ------------------------------

    def load(self):
        """
        Load all JSONL records and populate:
            • self.records          (metadata)
            • self.embeddings       (float32 matrix)

        Each JSONL line is expected to contain at least:
            id, category, source_path, chunk_index, text, embedding

        Additional keys (attack_type, section_type, etc.) are preserved.
        """

        if not self.path.exists():
            raise FileNotFoundError(f"Knowledge store missing: {self.path}")

        self.records = []
        vectors = []

        with self.path.open("r", encoding="utf-8") as f:
            for line in f:
                rec = json.loads(line)

                # Keep the full record so we don't lose fields like id/chunk_index
                self.records.append(rec)

                # Embeddings matrix for retrieval
                vectors.append(rec["embedding"])

        self.embeddings = np.array(vectors, dtype="float32")
        print(f"[KnowledgeStore] Loaded {len(self.records)} chunks from {self.path}.")

    # ------------------------ EMBED QUERY -----------------------------

    def embed_query(self, text: str) -> np.ndarray:
        """
        Convert a user query into an embedding vector.
        """
        resp = client.embeddings.create(model=EMBED_MODEL, input=[text])
        return np.array(resp.data[0].embedding, dtype="float32")

    # ------------------------ RETRIEVAL -------------------------------

    def retrieve(self, query: str, k: int = 5):
        """
        Return top-k most similar chunks to the query.

        Steps:
            1. Embed query
            2. Normalize query + store vectors
            3. Compute cosine similarity
            4. Rank and return top results

        Each result dict includes:
            score, id, category, attack_type, source, chunk_index,
            section_type, text
        """

        if self.embeddings is None:
            raise RuntimeError("Call .load() before retrieve().")

        # (1) Embed query
        q = self.embed_query(query)

        # (2) Normalize store + query
        norm_store = self.embeddings / np.linalg.norm(
            self.embeddings, axis=1, keepdims=True
        )
        norm_query = q / np.linalg.norm(q)

        # (3) Cosine similarity
        sims = norm_store @ norm_query

        # (4) Rank highest → lowest
        ranked = np.argsort(-sims)

        results = []
        for idx in ranked[:k]:
            rec = self.records[idx]
            results.append(
                {
                    "score": float(sims[idx]),
                    "id": rec.get("id"),  # e.g. "access_control::...::chunk_3"
                    "category": rec.get("category", "unknown"),
                    "attack_type": rec.get("attack_type"),
                    "source": rec.get("source_path", rec.get("source")),
                    "chunk_index": rec.get("chunk_index", 0),
                    "section_type": rec.get("section_type", "explanation"),
                    "text": rec.get("text", ""),
                }
            )

        return results